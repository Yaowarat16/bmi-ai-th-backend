import os
import torch
import urllib.request

_MODEL = None
DEVICE = "cpu"

# ====== ENV ======
# ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô deploy ‡∏ö‡∏ô Render
MODEL_URL = os.getenv("MODEL_URL")

# path ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î (Render ‡πÉ‡∏ä‡πâ /tmp ‡πÑ‡∏î‡πâ)
LOCAL_MODEL_PATH = "/tmp/bmi_render.pt"

# fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö local dev (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ MODEL_URL)
DEFAULT_LOCAL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(__file__)),
    "weights",
    "bmi_render.pt"
)


def _download_model(url: str, save_path: str):
    print(f"‚¨áÔ∏è Downloading model from: {url}")
    urllib.request.urlretrieve(url, save_path)
    print(f"‚úÖ Model downloaded to: {save_path}")


def load_model():
    print("üöÄ Loading TorchScript model...")
    print(f"üñ•Ô∏è DEVICE: {DEVICE}")

    # ====== ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ======
    if MODEL_URL:
        # üëâ ‡∏Å‡∏£‡∏ì‡∏µ Render / production
        if not os.path.exists(LOCAL_MODEL_PATH):
            _download_model(MODEL_URL, LOCAL_MODEL_PATH)
        model_path = LOCAL_MODEL_PATH
    else:
        # üëâ ‡∏Å‡∏£‡∏ì‡∏µ local dev
        model_path = DEFAULT_LOCAL_PATH

    print(f"üì¶ MODEL_PATH: {model_path}")

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"‚ùå Model file not found: {model_path}")

    # ====== Load TorchScript ======
    model = torch.jit.load(model_path, map_location=DEVICE)
    model.eval()

    print("‚úÖ Model loaded successfully (TorchScript)")
    return model


def get_model():
    """
    cache model (‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
    """
    global _MODEL
    if _MODEL is None:
        _MODEL = load_model()
    return _MODEL
